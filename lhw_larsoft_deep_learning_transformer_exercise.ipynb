{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise we're going to use a transformer encoder to classify tracks as either $\\mu^\\pm$, $\\pi^\\pm$ or proton. Compared to the code we wrote in the previous exercise, this notebook is a bit more like how I would write code in reality. It uses classes for a custom dataloader and for the custom network architecture."
      ],
      "metadata": {
        "id": "NEsdwTttulyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin with, we need to download the dataset"
      ],
      "metadata": {
        "id": "VTyXlYcNu3mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Load the track dataset:\n",
        "if not os.path.isfile('larsoft_workshop_tracks.tgz'):\n",
        "  !mkdir tracks\n",
        "  !wget --no-check-certificate 'https://www.hep.phy.cam.ac.uk/~lwhitehead/larsoft_workshop_tracks.tgz' -O larsoft_workshop_tracks.tgz\n",
        "  !tar -xzf larsoft_workshop_tracks.tgz -C tracks/"
      ],
      "metadata": {
        "id": "d3iW2PPgN92g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example we aren't looking at images, so we need to define our own dataset class. We create one by inheriting from the `torch.utils.data.Dataset` class, and we need to make sure to define the `__getitem__` and `__len__` functions."
      ],
      "metadata": {
        "id": "mlFSBHKkeSjr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZIJeU3gKM2w"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pickle\n",
        "import torch\n",
        "import csv\n",
        "\n",
        "# Useful class to hold all of the information that we have about our tracks\n",
        "class InputTrack():\n",
        "    def __init__(self, x, y, z, q, pdg, trackid, n_child_trk, n_child_shw, n_grandchild):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.z = z\n",
        "        self.q = q\n",
        "        self.pdg = pdg\n",
        "        self.trackid = trackid\n",
        "        self.n_child_trk = n_child_trk\n",
        "        self.n_child_shw = n_child_shw\n",
        "        self.n_grandchild = n_grandchild\n",
        "\n",
        "class TrackDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_path, file_name, sequence_length=128, transform=None):\n",
        "        self.file_path = file_path\n",
        "        self.input_files = []\n",
        "        self.get_input_files(file_name)\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    # Load the list of input file paths for the tracks\n",
        "    def get_input_files(self, file_name):\n",
        "        with open(self.file_path + file_name, 'r') as input_file_list:\n",
        "            self.input_files = [row[0] for row in csv.reader(input_file_list)]\n",
        "        print('Found',len(self.input_files),'tracks')\n",
        "\n",
        "    # Label values: 0 = muon, 1 = pion, 2 = proton\n",
        "    def convert_pdg_to_label(self, pdg):\n",
        "        if abs(pdg) == 13:\n",
        "            return 0\n",
        "        elif abs(pdg) == 211:\n",
        "            return 1\n",
        "        elif abs(pdg) == 2212:\n",
        "            return 2\n",
        "        else:\n",
        "            print('Track found with wrong pdg code... exiting')\n",
        "            exit()\n",
        "\n",
        "    # Function to pad short sequences at the start with values -1e9\n",
        "    def pad_sequence(self, sequence):\n",
        "        n_padding = self.sequence_length - sequence.size(0)\n",
        "        sequence = torch.nn.functional.pad(sequence, (n_padding, 0), value=-1e9)\n",
        "        return sequence\n",
        "\n",
        "    # Get the track of interest\n",
        "    def __getitem__(self, index):\n",
        "        track = None\n",
        "        # The pickle file location comes from the list of files\n",
        "        with open(self.file_path + self.input_files[index], 'rb') as f:\n",
        "            track = pickle.load(f)\n",
        "\n",
        "        # Convert the InputTrack information into torch tensors\n",
        "        x = torch.tensor(track.x, dtype=torch.float)\n",
        "        y = torch.tensor(track.y, dtype=torch.float)\n",
        "        z = torch.tensor(track.z, dtype=torch.float)\n",
        "        q = torch.tensor(track.q, dtype=torch.float)\n",
        "\n",
        "        n_hits = x.size(0)\n",
        "\n",
        "        # Pad (prepending the sequence) the tensors if we need to\n",
        "        if n_hits < self.sequence_length:\n",
        "            x = self.pad_sequence(x)\n",
        "            y = self.pad_sequence(y)\n",
        "            z = self.pad_sequence(z)\n",
        "            q = self.pad_sequence(q)\n",
        "        # Crop the tensors if we need to\n",
        "        elif n_hits > self.sequence_length:\n",
        "            x = x[(n_hits - self.sequence_length):]\n",
        "            y = y[(n_hits - self.sequence_length):]\n",
        "            z = z[(n_hits - self.sequence_length):]\n",
        "            q = q[(n_hits - self.sequence_length):]\n",
        "\n",
        "        # Get the class label from the pdg code\n",
        "        class_label = torch.tensor(self.convert_pdg_to_label(track.pdg), dtype=torch.long)\n",
        "        # Create the sequence of hits (charge is already normalised in the input)\n",
        "        data = torch.stack([x / 1000., y / 1000., z / 1000., q])\n",
        "        # Get the additional variables (empirical scaling for n_hits)\n",
        "        auxillary = torch.tensor([track.n_child_trk, track.n_child_shw,\n",
        "                                  track.n_grandchild, math.log10(n_hits) / 5.0],\n",
        "                                  dtype=torch.float32)\n",
        "\n",
        "        return (data.transpose(0,1), auxillary), class_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def GetDatasets(batch_size, sequence_length):\n",
        "    # Load the dataset and divide into train, validation and test samples, and\n",
        "    # set the sequence length for padding / truncating purposes\n",
        "    dataset = TrackDataset('tracks/','contained_track_files.txt', sequence_length)\n",
        "    indices = np.arange(len(dataset))\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # Define split points\n",
        "    train_idx, val_idx, test_idx = np.split(indices, [int(0.7*len(indices)), int(0.9*len(indices))])\n",
        "    print(len(train_idx), len(val_idx), len(test_idx))\n",
        "\n",
        "    # Create samplers\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "    val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "    test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
        "\n",
        "    # Create data loaders\n",
        "    training_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
        "\n",
        "    return training_loader, val_loader, test_loader\n",
        "\n",
        "# =============================================================================\n",
        "\n",
        "def count_particle_types(data_loader):\n",
        "\n",
        "    n_muons = 0\n",
        "    n_pions = 0\n",
        "    n_protons = 0\n",
        "    for (_, labels) in data_loader:\n",
        "        n_muons += (labels == 0).sum().item()\n",
        "        n_pions += (labels == 1).sum().item()\n",
        "        n_protons += (labels == 2).sum().item()\n",
        "\n",
        "    return n_muons, n_pions, n_protons"
      ],
      "metadata": {
        "id": "dBXvZbkFL1Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just load our data into the three Dataloaders and we can check what breakdown of event types we have in each sample."
      ],
      "metadata": {
        "id": "iw_twhSpSz7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset split into three dataloaders\n",
        "batch_size = 128 # Number of tracks processed in parallel\n",
        "sequence_length = 128 # Number of hits per track allowed\n",
        "\n",
        "# Load the datasets\n",
        "train_loader, val_loader, test_loader = ...\n",
        "\n",
        "n_train_muons, n_train_pions, n_train_protons = count_particle_types(train_loader)\n",
        "print(\"Training sample breakdown:\", count_particle_types(train_loader))\n",
        "print(\"Validation sample breakdown:\", count_particle_types(val_loader))\n",
        "print(\"Test sample breakdown:\", count_particle_types(test_loader))"
      ],
      "metadata": {
        "id": "R2mbmOvgNu-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to define our model. Previously we used the `torch.nn.Sequential` class to do this as it is straightfoward. It is, however, also quite limited in what you can do. Since each track has two sets of inputs we need to write our own custom model that inherits from the `torch.nn.Module` class.\n",
        "\n",
        "We'll need some different network layers in addition to those we used for the MLP and CNN:\n",
        "* `torch.nn.Embedding(num_embeddings, embedding_dim)`\n",
        "* `torch.nn.LayerNorm(normalized_shape)`\n",
        "* `torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, ..., batch_first)`\n",
        "* `torch.nn.TransformerEncoder(encoder_layer, num_layers)`\n",
        "* `torch.nn.AdaptiveAvgPool1d(dimension)`",
        "\n",
        "Note that in our case we need to set `batch_first=True` for our encoder layers"
      ],
      "metadata": {
        "id": "H8oBNB0xDIK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrackPIDNetwork(torch.nn.Module):\n",
        "    def __init__(self, n_features, n_classes, sequence_length, model_depth, n_heads,\n",
        "                 feed_forward_depth, n_encoder_layers, dropout, n_auxillary):\n",
        "        super(TrackPIDNetwork, self).__init__()\n",
        "\n",
        "        # Input mapping uses a linear layer to expand from n_features to model_depth\n",
        "        self.input_mapping = ...\n",
        "        # We use a learned embedding for the position encoding. This is basically\n",
        "        # a look-up table with sequence_length entries of size model_depth\n",
        "        self.position_encoding = ...\n",
        "\n",
        "        # Layer normalisation with shape equal to model_depth\n",
        "        self.layer_norm = ...\n",
        "\n",
        "        # The encoder itself. We have to define the encoder layers themselves\n",
        "        # before the actual entire encoder. The encoder is effectively a stack\n",
        "        # of the encoder layers\n",
        "        self.encoder_layer = ...\n",
        "        self.encoder = ...\n",
        "\n",
        "        # Adaptive pooling layer to reduced to a dimension of 1\n",
        "        self.pooling = ...\n",
        "\n",
        "        # Flatten layer\n",
        "        self.flatten = ...\n",
        "\n",
        "        # The outputs for particle id (n_classes)\n",
        "        self.classifier = ...\n",
        "\n",
        "    def forward(self, x, auxillary):\n",
        "        # Create the padding mask\n",
        "        padding_mask = self.create_mask(x)\n",
        "\n",
        "        # Embedding and position encoding\n",
        "        x = ...\n",
        "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0).repeat(x.size(0), 1)\n",
        "        x = ...\n",
        "\n",
        "        # Layer normalisation\n",
        "        x = ...\n",
        "\n",
        "        # Run the encoder, remembering we need to pass in the padding mask\n",
        "        x = ...\n",
        "\n",
        "        # Prepare for classification\n",
        "        x = x.transpose(2,1) # Need to swap model_depth and sequence dimensions\n",
        "        x = ... # After pooling the shape is now (batch, model_depth, 1)\n",
        "        x = ... # Flatten to remove the last dimension\n",
        "\n",
        "        # Concatenate with the auxillary variables\n",
        "        x = torch.cat((x, auxillary), dim=1)\n",
        "        # Final classification layer\n",
        "        output = ...\n",
        "\n",
        "        return output\n",
        "\n",
        "    def create_mask(self, x):\n",
        "        # Try making a mask\n",
        "        mask = (x == -1e9).all(dim=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "tKmR0BSFL2xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function performs one epoch of either training or validation, depending on the value of `is_training`. It is completely analagous to the code we wrote before but just encapsulated in a function for simplicity"
      ],
      "metadata": {
        "id": "deTp2PB5AgWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_epoch(epoch, data_loader, pid_lossfn, optimiser, initial_lr, is_training=True):\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Updating the learning rate is specific to training\n",
        "    if is_training:\n",
        "        my_model.train()\n",
        "        # With transformers it is often a good idea to \"warm up\". The easiest\n",
        "        # way to do this is to just ramp to your target learning rate. You'll\n",
        "        # see later that we'll choose 1e-3 as our learning rate, so here we\n",
        "        # slowly build up from 1e-4 to 1e-3 over the first 10 epochs\n",
        "        if epoch < 10:\n",
        "            lr = initial_lr * (epoch + 1) / 10\n",
        "            print(\"Learning rate warmup:\", lr)\n",
        "            for param_group in optimiser.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "        else:\n",
        "            cosine_scheduler.step()\n",
        "            for param_group in optimiser.param_groups:\n",
        "                print(\"Learning rate cosine annealing:\", param_group['lr'])\n",
        "                break\n",
        "    else:\n",
        "        my_model.eval()\n",
        "\n",
        "    # Loop over all of the batches in the dataset\n",
        "    for batch_no, (data, labels) in enumerate(data_loader):\n",
        "        data = (data[0].to(device), data[1].to(device))\n",
        "        labels = labels.to(device)\n",
        "        batch_loss = None\n",
        "\n",
        "        # If training then we need to remember to do the back propagation\n",
        "        if is_training:\n",
        "            outputs = my_model(data[0], data[1])\n",
        "            batch_loss = pid_lossfn(outputs, labels)\n",
        "            optimiser.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            optimiser.step()\n",
        "        # For validation we explicitly tell torch not to calculate gradients\n",
        "        # as it takes time and memory and isn't useful\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                outputs = my_model(data[0], data[1])\n",
        "                batch_loss = pid_lossfn(outputs, labels)\n",
        "\n",
        "        epoch_loss += batch_loss.item()\n",
        "\n",
        "    return epoch_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "-kOlL7VUCg7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get an instance of the network after defining some important parameters defining the specific characteristics that we want."
      ],
      "metadata": {
        "id": "edIyNQIJCtQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 4 # This is {x, y, z, q} for each hit\n",
        "n_classes = 3 # 0 = muon, 1 = pion, 2 = proton\n",
        "\n",
        "model_depth = 64 # Model depth of the transformer\n",
        "n_heads = 4 # Number of attention heads\n",
        "feed_forward_depth = 256 # Size of the feed-forward layer\n",
        "n_encoder_layers = 2 # Number of stacked encoders\n",
        "dropout = 0.3 # Dropout fraction used in the encoder\n",
        "n_auxillary = 4 # N track children, n shower children, n total descendants, n hits\n",
        "\n",
        "# Use a GPU if we have one available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using device', device)\n",
        "\n",
        "# Create the network with the required parameters\n",
        "my_model = ...\n",
        "\n",
        "model_parameters = filter(lambda p: p.requires_grad, my_model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print(\"Total parameters\",params)"
      ],
      "metadata": {
        "id": "wmg2NxIeCoae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some weights to deal with the class imbalance of the dataset\n",
        "pid_weights = torch.ones(3).to(device)\n",
        "pid_weights[1] = n_train_muons / n_train_pions\n",
        "pid_weights[2] = n_train_muons / n_train_protons\n",
        "print(pid_weights)\n",
        "\n",
        "# This time you need to use the `weight` argument with the cross entropy loss\n",
        "# function to help with class imbalance!\n",
        "pid_loss_fn = ...\n",
        "initial_lr = 1e-3 # N.B. 1e-4 is often a good choice for transformers\n",
        "# Let's use the AdamW optimiser again\n",
        "optimiser = torch.optim.AdamW(my_model.parameters(), lr=initial_lr)\n",
        "# Including this as an example of something that varies the learning rate. The\n",
        "# first transformer was trained using CosineAnnealing - it basically varies\n",
        "# the learning rate in a periodic fashion\n",
        "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=25)\n",
        "\n",
        "my_model.to(device)\n",
        "\n",
        "# Do the training and validation for n_epochs. Here I have set things up to keep\n",
        "# track of the epoch with the best validation loss so that we can stop if things\n",
        "# begin to get worse due to overtraining\n",
        "n_epochs = 10\n",
        "patience = 10\n",
        "training_losses = {}\n",
        "validation_losses = {}\n",
        "best_epoch_loss = 1e9\n",
        "best_epoch = -1\n",
        "best_epoch_state = None\n",
        "n_epoch_no_improvement = 0\n",
        "for e in range(0,n_epochs):\n",
        "    # Training\n",
        "    one_epoch_train_loss = one_epoch(e, train_loader, pid_loss_fn, optimiser, initial_lr, True)\n",
        "    # Validation\n",
        "    one_epoch_valid_loss = one_epoch(e, val_loader, pid_loss_fn, optimiser, initial_lr, False)\n",
        "    # Print the loss and store\n",
        "    print('Epoch', e, 'training loss =', one_epoch_train_loss, ' and validation loss', one_epoch_valid_loss)\n",
        "    training_losses[e] = one_epoch_train_loss\n",
        "    validation_losses[e] = one_epoch_valid_loss\n",
        "    # Keep track of the best epoch\n",
        "    if one_epoch_valid_loss < best_epoch_loss:\n",
        "        best_epoch = e\n",
        "        best_epoch_loss = one_epoch_valid_loss\n",
        "        best_epoch_state = my_model.state_dict()\n",
        "        n_epoch_no_improvement = 0\n",
        "    else:\n",
        "        n_epoch_no_improvement += 1\n",
        "\n",
        "    # If we haven't improved for a while then stop training\n",
        "    if n_epoch_no_improvement >= patience:\n",
        "        print(\"No improvement in validation loss for\", patience, \"epochs, stopping training\")\n",
        "        break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Eo3ONlBcLSDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we've used the validation loss to determine which epoch is the best, then we use an independent sample, here called the test sample, to actually benchmark the performance of the network. I've written the loop here as opposed to using the `one_epoch` function as I build a confusion matrix to show how well we have done"
      ],
      "metadata": {
        "id": "zcAeBdoPMqMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(\"Loading model state from best epoch (\", best_epoch, \")\")\n",
        "my_model.load_state_dict(best_epoch_state)\n",
        "\n",
        "# Run the test sample\n",
        "test_loss = 0.0\n",
        "\n",
        "# Make sure we are in evaluation mode without gradients\n",
        "my_model.eval()\n",
        "test_confusion_matrix = np.zeros((3,3), dtype=float)\n",
        "with torch.no_grad():\n",
        "    for batch_no, (data, labels) in enumerate(test_loader):\n",
        "        data = (data[0].to(device), data[1].to(device))\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = my_model(data[0], data[1])\n",
        "        batch_loss = pid_loss_fn(outputs, labels)\n",
        "        test_loss += batch_loss.item()\n",
        "\n",
        "        outputs_as_class_cpu = outputs.argmax(dim=1).cpu()\n",
        "        labels_cpu = labels.cpu()\n",
        "\n",
        "        test_confusion_matrix += confusion_matrix(outputs_as_class_cpu.numpy(),\n",
        "                                                  labels_cpu.numpy(),\n",
        "                                                  labels=np.arange(3))\n",
        "\n",
        "test_loss = test_loss / len(test_loader)\n",
        "\n",
        "print(test_loss)\n",
        "print(test_confusion_matrix)"
      ],
      "metadata": {
        "id": "0wXI81OYLrp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This last bit of code just normalises the matrix per column so that you can see what fraction of muons, pions and protons were classifed as each class. At least in HEP we'd describe the diagonal of the matrix as the efficiency."
      ],
      "metadata": {
        "id": "RgQzQmE8SC5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also normalise this matrix by row to see what percentage of each true\n",
        "# class tracks are classified as each of the three classes.\n",
        "n_test_muons, n_test_pions, n_test_protons = count_particle_types(test_loader)\n",
        "test_confusion_matrix[:,0] /= n_test_muons\n",
        "test_confusion_matrix[:,1] /= n_test_pions\n",
        "test_confusion_matrix[:,2] /= n_test_protons\n",
        "print(test_confusion_matrix)"
      ],
      "metadata": {
        "id": "2so_3PqstowK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kytiHfonPutD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
