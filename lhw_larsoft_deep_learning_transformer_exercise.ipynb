{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this note book we will attempt to classify a set of neutrino interactions as either CC $\\nu_\\mu$, CC $\\nu_e$ and NC $\\nu$ events using a transformer encoder"
      ],
      "metadata": {
        "id": "U1qY-EtAtudC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYOqH5PkUaO5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"torchvision version:\", torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the dataset. This is a sample of 30,000 images from a simple LArTPC simulation using GENIE input neutrino events containing equal numbers of CC $\\nu_\\mu$, CC $\\nu_e$ and NC $\\nu$ interactions. It will save the `.png` images to the `images` directory."
      ],
      "metadata": {
        "id": "E7n7xIsOuFR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Load the neutrino dataset:\n",
        "if not os.path.isfile('images/images.tgz'):\n",
        "  !mkdir images\n",
        "  !wget --no-check-certificate 'https://www.hep.phy.cam.ac.uk/~lwhitehead/genie_neutrino_images.tgz' -O images/images.tgz\n",
        "  !tar -xzf images/images.tgz -C images/\n",
        "\n",
        "# Work out the number of classes form the directory structure\n",
        "root_dir = 'images/'\n",
        "dir_contents = os.listdir(root_dir)\n",
        "num_classes = sum(os.path.isdir(os.path.join(root_dir, item)) for item in dir_contents)\n",
        "\n",
        "print('Dataset consists of', num_classes, 'classes')\n",
        "\n",
        "class_names = ['CC numu', 'CC nue', 'NC']\n",
        "for c in range(num_classes):\n",
        "  print('Number of',class_names[c],'images:')\n",
        "  !ls -1 images/$c/*.png | wc -l"
      ],
      "metadata": {
        "id": "RXF_mg2WVk3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to manipulate the input images a bit to get them into the prefered format. We also downsample them by a factor of two for convenience here (to save time for training the networks)"
      ],
      "metadata": {
        "id": "8Ayye5Q8u92c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# We need to define a transform to resize and scale the images when loaded\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((112, 112)),   # reduce size (they are 224 x 224)\n",
        "    torchvision.transforms.ToTensor(),           # convert to tensor [0,1]\n",
        "    torchvision.transforms.Lambda(lambda x: x[2].unsqueeze(0)) # extract the w view\n",
        "])\n",
        "\n",
        "# Now we can use a torchvision dataset to load these images\n",
        "dataset = torchvision.datasets.ImageFolder(root=\"images/\", transform=transform)\n",
        "print(\"Dataset classes:\", dataset.classes)       # list of class names (sorted by folder name)\n",
        "\n",
        "# Now we need to divide this into train and validation dataloader objects\n",
        "np.random.seed(24601)\n",
        "indices = np.arange(len(dataset))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Define split points\n",
        "train_idx, val_idx, = np.split(indices, [int(0.7*len(indices))])\n",
        "print(\"Using\", len(train_idx), \"images for training and\", len(val_idx), \"for validation\")\n",
        "\n",
        "# Create samplers\n",
        "train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, num_workers=2)"
      ],
      "metadata": {
        "id": "JaMyZL-ZXAWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is always a good idea to visualise your data to make sure it looks how you expect it to. With image-based inputs then it is especially easy to do this!"
      ],
      "metadata": {
        "id": "wspmdRHeu7JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numu_event = dataset.__getitem__(1)\n",
        "print('True class:', numu_event[1])\n",
        "fig, axes = plt.subplots(1,3)\n",
        "axes[0].imshow(numu_event[0][0])\n",
        "\n",
        "nue_event = dataset.__getitem__(10001)\n",
        "print('True class:', nue_event[1])\n",
        "axes[1].imshow(nue_event[0][0])\n",
        "\n",
        "nc_event = dataset.__getitem__(20001)\n",
        "print('True class:', nc_event[1])\n",
        "axes[2].imshow(nc_event[0][0])"
      ],
      "metadata": {
        "id": "5oKxh41q0Ikq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define our transformer encoder. This is a bit more complicated that before because we need to divide our images up into patches so that they can be input to a sequential-style model. Follow the comments in the code below, but as a brief overview, the new network layers of interest here are:\n",
        "* `torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, ..., batch_first)`\n",
        "* `torch.nn.TransformerEncoder(encoder_layer, num_layers)`\n",
        "* `torch.nn.AdaptiveAvgPool1d(dimension)`"
      ],
      "metadata": {
        "id": "Y06NLPTE2Xpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's try making a transformer encoder to do the same job. We'll need to\n",
        "# break the image up into patches in order to encode it for the transformer\n",
        "patch_dim = 14\n",
        "# Calculate the sequence length (= number of patches)\n",
        "seq_length = ...\n",
        "print('Number of patches =', seq_length)\n",
        "# Set the model depth to 64\n",
        "model_depth = 64\n",
        "\n",
        "# Small class to allow us to do a transpose of a tensor in torch.nn.Sequential\n",
        "# This is needed to change the position of the patch dimension in the tensor\n",
        "class Transpose(torch.nn.Module):\n",
        "    def __init__(self, dim1, dim2):\n",
        "        super().__init__()\n",
        "        self.dim1, self.dim2 = dim1, dim2\n",
        "    def forward(self, x):\n",
        "        return x.transpose(self.dim1, self.dim2)\n",
        "\n",
        "# We have to define the transformer encoder layer too. We have to set the\n",
        "# batch_first argument to True, since for some reason the PyTorch implementation\n",
        "# has the batch last, meaning we'd need to perform a transpose otherwise.\n",
        "# We need to set the model depth and the number of attention heads, let's use\n",
        "# two for the latter. A typical choice for the feed-forward network dimension\n",
        "# is four times the model depth\n",
        "encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "    ..., # Set the model depth\n",
        "    ..., # Set the number of attention heads\n",
        "    ..., # Set the feed-forward network dimension\n",
        "    batch_first=True\n",
        ")\n",
        "\n",
        "# Now for the sequential model itself. Note that a lot of this is specific\n",
        "# to a Vision Transformer (Encoder) as opposed to what you would use for\n",
        "# generic sequence information\n",
        "transformer_model = torch.nn.Sequential(\n",
        "    # This first convolution is a neat trick to do the patching and encoding\n",
        "    # for us in a single layer\n",
        "    torch.nn.Conv2d(1, model_depth, kernel_size=patch_dim, stride=patch_dim),\n",
        "    # Flatten the tensor in the second dimension\n",
        "    ...,\n",
        "    # Transpose dimensions 1 and 2 for the encoder\n",
        "    Transpose(1,2),\n",
        "    # Create the encoder from the encoder layer, and choose two such layers\n",
        "    ...,\n",
        "    # Transpose back to the original dimension order\n",
        "    Transpose(2,1),\n",
        "    # Adaptive average pooling pooling layer applied to dimension 1\n",
        "    ...,   # (batch, model_depth, 1)\n",
        "    # Flatten the tensor in dimension 1 this time\n",
        "    ...,             # (batch, model_depth)\n",
        "    # Final linear layer to predict the three classes\n",
        "    ...\n",
        "    # Expected softmax activation is implicit in the loss function\n",
        ")\n",
        "\n",
        "# Let's pass an image through the network just to check the output\n",
        "# is of the expected shape\n",
        "test_image = dataset.__getitem__(0)[0]\n",
        "test_image = test_image.unsqueeze(0)\n",
        "print(test_image.shape)\n",
        "print(transformer_model(test_image).shape)\n",
        "\n",
        "n_params = sum(p.numel() for p in transformer_model.parameters() if p.requires_grad)\n",
        "print('Number of trainable parameters =', n_params)"
      ],
      "metadata": {
        "id": "A3SxsIiCKQXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a GPU if we request one\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using device', device)\n",
        "transformer_model.to(device)"
      ],
      "metadata": {
        "id": "q-bGUyRqdxGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare for training\n",
        "learning_rate = 0.0001\n",
        "transformer_loss_fn = torch.nn.CrossEntropyLoss()\n",
        "transformer_optimiser = torch.optim.AdamW(transformer_model.parameters(), lr=learning_rate, weight_decay=1e-2)"
      ],
      "metadata": {
        "id": "7KzTfk6FOaU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(0, n_epochs):\n",
        "  transformer_model.train()\n",
        "  running_loss = 0.0\n",
        "  for (images, labels) in train_loader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = transformer_model(images)\n",
        "    loss = transformer_loss_fn(outputs, labels)\n",
        "\n",
        "    # Backward pass and optimisation\n",
        "    transformer_optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    transformer_optimiser.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  # Validation\n",
        "  running_val_loss = 0.0\n",
        "  transformer_model.eval()\n",
        "  with torch.no_grad():\n",
        "    for (images, labels) in val_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Make the predictions\n",
        "      outputs = transformer_model(images)\n",
        "      loss = transformer_loss_fn(outputs, labels)\n",
        "      running_val_loss += loss.item()\n",
        "\n",
        "  print(\"Epoch\", epoch, \"training loss:\", running_loss/len(train_loader), \"validation loss:\", running_val_loss/len(val_loader))"
      ],
      "metadata": {
        "id": "E2m3EIkMJZqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a list of incorrect classifications\n",
        "def get_incorrect_classifications(model, dataloader):\n",
        "  incorrect_indices = []\n",
        "  with torch.no_grad():\n",
        "    for (images, labels) in dataloader:\n",
        "      images = images.to(device)\n",
        "      predictions = model(images).cpu().numpy()\n",
        "\n",
        "      for i in range(len(labels)):\n",
        "        prediction = np.argmax(predictions[i])\n",
        "        truth = labels[i].numpy()\n",
        "        if prediction != truth:\n",
        "          image = images[i].cpu().numpy()\n",
        "          image = image.transpose([1,2,0])\n",
        "          incorrect_indices.append([image, prediction, truth])\n",
        "\n",
        "  print('Accuracy =',1 - len(incorrect_indices)/len(val_idx))\n",
        "  return incorrect_indices\n",
        "\n",
        "# Now you can modify this part to draw different images from the failures list\n",
        "# You can change the value of im to look at different failures\n",
        "def draw_event(incorrect_indices, index):\n",
        "  image_to_plot = incorrect_indices[index][0]\n",
        "  image_to_plot = np.clip(image_to_plot, 0.0, 1.0)\n",
        "  fig, ax = plt.subplots(1, 1)\n",
        "  print('Incorrect classification for image',index,\n",
        "        ': predicted =',incorrect_indices[index][1],\n",
        "        'with true =',incorrect_indices[index][2])\n",
        "  ax.imshow(image_to_plot)"
      ],
      "metadata": {
        "id": "6PLu0aaMvFLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_transformer_indices = get_incorrect_classifications(transformer_model, val_loader)"
      ],
      "metadata": {
        "id": "BIJYiU6GJogw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_event(incorrect_transformer_indices, 2)"
      ],
      "metadata": {
        "id": "VIvbXSC0Xy_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "35atv2SUX4Au"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
