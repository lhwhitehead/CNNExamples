{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this exercise is to create a simple MLP and CNN to solve image classification problems. You can compare the performance of the networks and understand the behaviour given what was presented in the lectures"
      ],
      "metadata": {
        "id": "5ulMurEy1tQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU_GuShCusOz"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist, cifar10, cifar100\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "print('Tensorflow version:',tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "You can use the function below to load some of the simple datasets available directly from `keras`. There are three options for the `dataset_name` argument:\n",
        "1. `mnist`: a dataset of handwritten digits from 0 - 9. These images are (28,28,1) in shape\n",
        "2. `cifar10`: these are small colour images with shape (32,32,3) from ten different classes (plane, car, bird, cat, deer, dog, frog, horse, ship, truck)\n",
        "3. `cifar100`: as above but with 100 classes! This will not be feasible to use with just a CPU as it would take a fairly complex network with many parameters. I have included it in case you want to play with this on a GPU one day"
      ],
      "metadata": {
        "id": "uBXkX4Ki2UoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset_name='mnist'):\n",
        "  # MNIST, CIFAR10 and CIFAR100 are standard datasets we can load straight\n",
        "  # from keras. The data are split between train and test sets automatically\n",
        "  # - x_train is a numpy array that stores the training images\n",
        "  # - y_train is a numpy array that stores the true class of the training images\n",
        "  # - x_train is a numpy array that stores the testing images\n",
        "  # - y_train is a numpy array that stores the true class of the testing images\n",
        "  if dataset_name.lower() == 'cifar10':\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    n_classes = 10\n",
        "  elif dataset_name.lower() == 'cifar100':\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    n_classes = 100\n",
        "  elif dataset_name.lower() == 'mnist':\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    # MNIST is greyscale so we have to do a trick to add a depth dimension\n",
        "    x_train = np.expand_dims(x_train, axis=-1)\n",
        "    x_test = np.expand_dims(x_test, axis=-1)\n",
        "    n_classes = 10\n",
        "  else:\n",
        "    print('Requested dataset does not exist. Please choose from mnist, cifar10 or cifar100')\n",
        "    return\n",
        "\n",
        "  # Let's check the shape of the images for convenience\n",
        "  print(\"Shape of x_train =\",x_train.shape)\n",
        "  print(\"Shape of x_test =\",x_test.shape)\n",
        "\n",
        "  # The y_train and y_test values we loaded also need to be modified.\n",
        "  # These values store the true classification of the images (0-9) as a single\n",
        "  # number. We need to convert the single value into an array of length 10\n",
        "  # corresponding to the number of output classes. Thus values of\n",
        "  # y = 2 becomes y = [0,0,1,0,0,0,0,0,0,0]\n",
        "  # y = 8 becomes y = [0,0,0,0,0,0,0,0,1,0]\n",
        "  y_train = keras.utils.to_categorical(y_train, n_classes)\n",
        "  y_test = keras.utils.to_categorical(y_test, n_classes)\n",
        "\n",
        "  print(\"Shape of y_train =\", y_train.shape)\n",
        "  print(\"Shape of y_test =\", y_test.shape)\n",
        "\n",
        "  # Let's take a look at a few example images from the training set\n",
        "  n_plots=5\n",
        "  fig, ax = plot.subplots(1, n_plots)\n",
        "  for plot_number in range (0, n_plots):\n",
        "    ax[plot_number].imshow(x_train[plot_number])\n",
        "\n",
        "  return (x_train, y_train), (x_test, y_test), n_classes"
      ],
      "metadata": {
        "id": "jVS-09BUvC2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Here we use the `load_data` function to load our dataset. In the\n",
        "first instance we will use `mnist` since it is the simplest dataset and we can use a very simple CNN."
      ],
      "metadata": {
        "id": "CpW7YAi4LEo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the input data.\n",
        "# x_train is the training data, and y_train the corresponding true labels\n",
        "# x_test is the testing data, and y_test the corresponding true labels\n",
        "# We don't have a separate validation sample in these keras datasets\n",
        "# Num_classes is the number of true classes\n",
        "(x_train, y_train), (x_test, y_test), num_classes = load_dataset('mnist')"
      ],
      "metadata": {
        "id": "4-x32SkYvHVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting our CNN, let's make a simple MLP to see how well it does. MLPs consist of a number of fully connected (or dense) layers. We need to make sure that we flatten the input in this case since we have images. We'll make a network with three dense layers (256, 128 and 64 neurons) interspersed with dropout layers (fraction 0.25), and then the final dense layer for classification.\n",
        "\n",
        "* Flatten layer: `keras.layers.Flatten()`\n",
        "\n",
        "* Dense layer: `keras.layers.Dense(num_nodes, activation='relu')` where the num_nodes is how many neurons are in the layer. The final layer of the model needs have to have `num_nodes = num_classes`, and should use the `softmax` activation\n",
        "\n",
        "* Dropout layer: `keras.layers.Dropout(fraction)`\n",
        "\n",
        "Printing a summary of the network should give you the following:\n",
        "```Model: \"functional\"\n",
        "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
        "┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n",
        "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
        "│ input_layer_1 (InputLayer)           │ (None, 28, 28, 1)           │               0 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ flatten_1 (Flatten)                  │ (None, 784)                 │               0 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dense_2 (Dense)                      │ (None, 256)                 │         200,960 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dropout_1 (Dropout)                  │ (None, 256)                 │               0 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dense_3 (Dense)                      │ (None, 128)                 │          32,896 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dropout_2 (Dropout)                  │ (None, 128)                 │               0 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dense_4 (Dense)                      │ (None, 64)                  │           8,256 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dropout_3 (Dropout)                  │ (None, 64)                  │               0 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dense_5 (Dense)                      │ (None, 10)                  │             650 │\n",
        "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
        " Total params: 242,762 (948.29 KB)\n",
        " Trainable params: 242,762 (948.29 KB)\n",
        " Non-trainable params: 0 (0.00 B)\n"
      ],
      "metadata": {
        "id": "gg1VdJFN9xed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our MLP: replace \"None\" with the corresponding layers as described\n",
        "input_layer = keras.layers.Input(x_train[0].shape)\n",
        "# First layer: flatten the 2D input into 1D for the dense layers\n",
        "x = None(...)(input_layer)\n",
        "# Second layer: dense layer with 256 neurons and a relu activation\n",
        "x = None(...)(x)\n",
        "# Third layer: dropout with 25% of neurons disabled\n",
        "x = None(...)(x)\n",
        "# Fourth layer: dense layer with 128 neurons and a relu activation\n",
        "x = None(...)(x)\n",
        "# Fifth layer: dropout with 25% of neurons disabled\n",
        "x = None(...)(x)\n",
        "# Sixth layer: dense layer with 64 neurons and a relu activation\n",
        "x = None(...)(x)\n",
        "# Seventh layer: dropout with 25% of neurons disabled\n",
        "x = None(...)(x)\n",
        "# Eighth layer: dense layer for classification into the number of classes\n",
        "x = None(...)(x)\n",
        "# Define the model from the input and final layers\n",
        "mlp_model = keras.Model(input_layer, x)\n",
        "# Print the model summary\n",
        "mlp_model.summary()"
      ],
      "metadata": {
        "id": "e2aLYW9A-PK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to define the loss function and optimiser that we will use to perform the gradient descent optimisation.\n",
        "\n",
        "* `keras.losses.categorical_crossentropy` is the loss function for multi-category classification tasks\n",
        "* `keras.optimizers.Adam(learning_rate=<learning_rate>)` is a choice of optimiser that can be used here. We need to give the learning rate as an argument\n",
        "\n",
        "The next step is to then compile the model and tell it which loss function and optimiser to use, and which metrics to display whilst training.\n",
        "\n",
        "* `model.compile(loss=<loss_function>, optimizer=<optimiser>, metrics=['accuracy'])`, to give an example where we will see the accuracy during the training process."
      ],
      "metadata": {
        "id": "hB4vgwWge2Yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The batch size controls the number of images that are processed simultaneously\n",
        "batch_size = 128\n",
        "# The number of epochs that we want to train the network for\n",
        "epochs = 5\n",
        "# The learning rate (step size in gradient descent)\n",
        "learning_rate = 0.001\n",
        "# Define the loss function and optimiser and then compile the model, replacing\n",
        "# \"None\" as required\n",
        "# Categorical crossentropy loss function\n",
        "loss_function = None\n",
        "# Adam optimiser using the learning rate defined above\n",
        "optimiser = None\n",
        "# Compile the model with the loss function and optimisers defined above\n",
        "None"
      ],
      "metadata": {
        "id": "PNVlmI_lBuWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train our MLP and run it on the MNIST dataset. We do this using the `fit` function of the model. It has many arguments, of which I list those we will need below:\n",
        "\n",
        "* `model.fit(x=<x>, y=<y>, batch_size=<batch_size>, epochs=<epochs>, validation_data = (<x_test>, <y_test>), verbose=1)`"
      ],
      "metadata": {
        "id": "KvxN5p94CTB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the training images and targets, and use the test\n",
        "# images as the validation sample. Replace \"None\" as appropriate\n",
        "None"
      ],
      "metadata": {
        "id": "eEcATfMwCXT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a couple of functions to look at some images that we classified incorrectly"
      ],
      "metadata": {
        "id": "V6Gy8ZYKwoGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a list of incorrect classifications\n",
        "def FindIncorrectClassifications(network_model, images, targets):\n",
        "    incorrect_indices = []\n",
        "    # Use the network to predict the classification of the images.\n",
        "    raw_predictions = network_model.predict(images)\n",
        "    for i in range(0, len(raw_predictions)):\n",
        "        # Remember the raw output from the CNN gives us an array of scores. We want\n",
        "        # to select the highest one as our prediction. We need to do the same thing\n",
        "        # for the truth too since we converted our numbers to a categorical\n",
        "        # representation earlier. We use the np.argmax() function for this\n",
        "        prediction = np.argmax(raw_predictions[i])\n",
        "        truth = np.argmax(targets[i])\n",
        "        if prediction != truth:\n",
        "            incorrect_indices.append([i,prediction,truth])\n",
        "    print('Number of images that were incorrectly classified =',len(incorrect_indices))\n",
        "    return incorrect_indices\n",
        "\n",
        "def DrawFailure(images, incorrect_indices, index_to_show=0):\n",
        "    image_to_plot = images[incorrect_indices[index_to_show][0]]\n",
        "    fig, ax = plot.subplots(1, 1)\n",
        "    print('Incorrect classification for image',incorrect_indices[index_to_show][0],\n",
        "          ': predicted =',incorrect_indices[index_to_show][1],\n",
        "          'with true =',incorrect_indices[index_to_show][2])\n",
        "    ax.imshow(image_to_plot)"
      ],
      "metadata": {
        "id": "ctsu8BBhwq7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now lets look at the images"
      ],
      "metadata": {
        "id": "NtVg8M0kxszf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_failures = None"
      ],
      "metadata": {
        "id": "Rb_vr82vx7tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DrawFailure(x_test, mlp_failures, 0)"
      ],
      "metadata": {
        "id": "G5gQzJRtxji0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now we want to define a CNN. The basic building blocks you will need are:\n",
        "\n",
        "\n",
        "*   Convolutional layers: `keras.layers.Conv2D(num_filters, (k,k), activation='relu')`. Typical values of `k` are 3, 5, or 7\n",
        "*   Pooling layers: `keras.layers.MaxPooling2D((2,2))` will perform a factor of 2 downsampling in the two dimensions of image\n",
        "*   Dropout: keras.layers.Dropout(fraction) where fraction is the fraction of weights that are ignored. Typical values can be 0.25 or 0.5\n",
        "*   Dense layers: `keras.layers.Dense(num_nodes, activation='relu')` where the num_nodes is how many neurons are in the layer. The final layer of the CNN needs have to have `num_nodes = num_classes`\n",
        "*   Flatten layer: This just converts and n-dimensional tensor into a vector. In this case we use it to present a dense output layer with a vector input\n",
        "\n",
        "\n",
        "In the following way of writing our network, we need to write things in the form:\n",
        "\n",
        "`layer_output = keras.layers.LayerNameHere(arguments_go_here)(layer_input)`\n",
        "\n",
        "For the first CNN we are building, you will hopefully see the following output from the model.summary() command:\n",
        "\n",
        "```\n",
        "Model: \"model\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        " input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
        "                                                                 \n",
        " conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
        "                                                                 \n",
        " max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
        "                                                              \n",
        " dropout (Dropout)            (None, 13, 13, 32)        0         \n",
        "                                                                 \n",
        " flatten (Flatten)            (None, 5408)              0         \n",
        "                                                                 \n",
        " dense (Dense)                (None, 10)                54090     \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 54,410\n",
        "Trainable params: 54,410\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```"
      ],
      "metadata": {
        "id": "687KJL-5B_8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our simple CNN model: replace \"None\" with the corresponding layers as described\n",
        "input_layer = keras.layers.Input(x_train[0].shape)\n",
        "# First layer: 2D convolution with 32 filters of size (3,3) and relu activation\n",
        "x = None(...)(input_layer)\n",
        "# Second layer: 2D max pooling layer to downsample by a factor of 2 in both dimensions\n",
        "x = None(...)(x)\n",
        "# Third layer: dropout with 25% of neurons disabled\n",
        "x = None(...)(x)\n",
        "# Fourth layer: flatten the output into 1D for input to a dense layer\n",
        "x = None(...)(x)\n",
        "# Fifth layer: dense layer for classification into the number of classes\n",
        "x = None(...)(x)\n",
        "# Define the model from the input and final layers\n",
        "cnn_model = keras.Model(input_layer, x)\n",
        "# Print the model summary\n",
        "cnn_model.summary()"
      ],
      "metadata": {
        "id": "GJo_UGUdmJ-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the model to train with the same hyperparameters as the MLP\n",
        "cnn_loss_function = keras.losses.categorical_crossentropy\n",
        "cnn_optimiser = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "cnn_model.compile(loss=cnn_loss_function, optimizer=cnn_optimiser, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PMNZJzKcMdap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can run our network on whichever data sample we requested. Initially on `mnist` we'll hopefully see that we can reach a very high accuracy."
      ],
      "metadata": {
        "id": "HIYp4y4WRrcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the training images and targets, and use the test\n",
        "# images as the validation sample.\n",
        "cnn_model.fit(x_train, y_train,\n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          verbose = 1,\n",
        "          validation_data = (x_test, y_test))\n"
      ],
      "metadata": {
        "id": "Oi9PWqaExzFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at some failures again"
      ],
      "metadata": {
        "id": "S1dtMTOHyB5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_failures = FindIncorrectClassifications(cnn_model, x_test, y_test)"
      ],
      "metadata": {
        "id": "y5Jtijt6yE-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DrawFailure(x_test, cnn_failures, 0)"
      ],
      "metadata": {
        "id": "Y38_7gQGyJAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now we will make a CNN using the depthwise-separable convolution layer.\n",
        "* `keras.layers.SeparableConv2D(num_filters, kernel_size, activation)` noting that there are many other arguments. The number of filters corresponds to the point-wise convolution that sets the number of output channels. The kernel size corresponds to the size of the kernel in the initial depthwise convolution.\n",
        "\n",
        "In this case, we want to keep the output data size from the SeparableConv2D layer the same as our previous Conv2D layer, so we need to have the same number of filters and the same kernel size.\n",
        "\n",
        "The network summary should look as follows:\n",
        "\n",
        "\n",
        "```\n",
        "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
        "┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n",
        "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
        "│ input_layer_2 (InputLayer)           │ (None, 28, 28, 1)           │               0 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ separable_conv2d (SeparableConv2D)   │ (None, 26, 26, 32)          │              73 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ max_pooling2d_1 (MaxPooling2D)       │ (None, 13, 13, 32)          │               0 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dropout_4 (Dropout)                  │ (None, 13, 13, 32)          │               0 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ flatten_2 (Flatten)                  │ (None, 5408)                │               0 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dense_5 (Dense)                      │ (None, 10)                  │          54,090 │\n",
        "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
        " Total params: 54,163 (211.57 KB)\n",
        " Trainable params: 54,163 (211.57 KB)\n",
        " Non-trainable params: 0 (0.00 B)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qOATUAeB53EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our mobile CNN model: replace \"None\" with the corresponding layers as described\n",
        "input_layer = keras.layers.Input(x_train[0].shape)\n",
        "# First layer: 2D depthwise separable convolution with 32 point-wise filters and (3,3) kernels and relu activation\n",
        "x = None(...)(input_layer)\n",
        "# Second layer: 2D max pooling layer to downsample by a factor of 2 in both dimensions\n",
        "x = None(...)(x)\n",
        "# Third layer: dropout with 25% of neurons disabled\n",
        "x = None(...)(x)\n",
        "# Fourth layer: flatten the output into 1D for input to a dense layer\n",
        "x = None(...)(x)\n",
        "# Fifth layer: dense layer for classification into the number of classes\n",
        "x = None(...)(x)\n",
        "# Define the model from the input and final layers\n",
        "mobile_model = keras.Model(input_layer, x)\n",
        "# Print the model summary\n",
        "mobile_model.summary()"
      ],
      "metadata": {
        "id": "s3NxXKPd59Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the model to train with the same hyperparameters as the MLP\n",
        "mobile_loss_function = keras.losses.categorical_crossentropy\n",
        "mobile_optimiser = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "mobile_model.compile(loss=mobile_loss_function, optimizer=mobile_optimiser, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ilOtq5-062QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the training images and targets, and use the test\n",
        "# images as the validation sample.\n",
        "mobile_model.fit(x_train, y_train,\n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          verbose = 1,\n",
        "          validation_data = (x_test, y_test))"
      ],
      "metadata": {
        "id": "Ld56SGCL67Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to look at errors again if you want to"
      ],
      "metadata": {
        "id": "trxAzWq_zdPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobile_failures = FindIncorrectClassifications(mobile_model, x_test, y_test)"
      ],
      "metadata": {
        "id": "eTY-jYElz9j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DrawFailure(x_test, mobile_failures, 0)"
      ],
      "metadata": {
        "id": "fU2sVHLXzkfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1iTz8SZU02bq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}