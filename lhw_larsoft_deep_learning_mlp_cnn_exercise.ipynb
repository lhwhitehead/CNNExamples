{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this note book we will attempt to classify a set of neutrino interactions as either CC $\\nu_\\mu$, CC $\\nu_e$ and NC $\\nu$ events using a Multi-Layer Perceptron and a CNN."
      ],
      "metadata": {
        "id": "VKn84YG-tnCh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYOqH5PkUaO5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"torchvision version:\", torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the dataset. This is a sample of 30,000 images from a simple LArTPC simulation using GENIE input neutrino events containing equal numbers of CC $\\nu_\\mu$, CC $\\nu_e$ and NC $\\nu$ interactions. It will save the `.png` images to the `images` directory."
      ],
      "metadata": {
        "id": "UgfSLZf7t1ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Load the neutrino dataset:\n",
        "if not os.path.isfile('images/images.tgz'):\n",
        "  !mkdir images\n",
        "  !wget --no-check-certificate 'https://www.hep.phy.cam.ac.uk/~lwhitehead/genie_neutrino_images.tgz' -O images/images.tgz\n",
        "  !tar -xzf images/images.tgz -C images/\n",
        "\n",
        "# Work out the number of classes form the directory structure\n",
        "root_dir = 'images/'\n",
        "dir_contents = os.listdir(root_dir)\n",
        "num_classes = sum(os.path.isdir(os.path.join(root_dir, item)) for item in dir_contents)\n",
        "\n",
        "print('Dataset consists of', num_classes, 'classes')\n",
        "\n",
        "class_names = ['CC numu', 'CC nue', 'NC']\n",
        "for c in range(num_classes):\n",
        "  print('Number of',class_names[c],'images:')\n",
        "  !ls -1 images/$c/*.png | wc -l"
      ],
      "metadata": {
        "id": "RXF_mg2WVk3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to manipulate the input images a bit to get them into the prefered format. We also downsample them by a factor of two for convenience here (to save time for training the networks)"
      ],
      "metadata": {
        "id": "Gnw51noVuIqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# We need to define a transform to resize and scale the images when loaded\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((112, 112)),   # reduce size (they are 224 x 224)\n",
        "    torchvision.transforms.ToTensor(),           # convert to tensor [0,1]\n",
        "    torchvision.transforms.Lambda(lambda x: x[2].unsqueeze(0)) # extract the w view\n",
        "])\n",
        "\n",
        "# Now we can use a torchvision dataset to load these images\n",
        "dataset = torchvision.datasets.ImageFolder(root=\"images/\", transform=transform)\n",
        "print(\"Dataset classes:\", dataset.classes)       # list of class names (sorted by folder name)\n",
        "\n",
        "# Now we need to divide this into train and validation dataloader objects\n",
        "np.random.seed(24601)\n",
        "indices = np.arange(len(dataset))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Define split points\n",
        "train_idx, val_idx, = np.split(indices, [int(0.7*len(indices))])\n",
        "print(\"Using\", len(train_idx), \"images for training and\", len(val_idx), \"for validation\")\n",
        "\n",
        "# Create samplers\n",
        "train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, num_workers=2)"
      ],
      "metadata": {
        "id": "JaMyZL-ZXAWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is always a good idea to visualise your data to make sure it looks how you expect it to. With image-based inputs then it is especially easy to do this!"
      ],
      "metadata": {
        "id": "OF0da_ROup3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numu_event = dataset.__getitem__(0)\n",
        "print('True class:', numu_event[1])\n",
        "fig, axes = plt.subplots(1,3)\n",
        "axes[0].imshow(numu_event[0][0])\n",
        "\n",
        "nue_event = dataset.__getitem__(10000)\n",
        "print('True class:', nue_event[1])\n",
        "axes[1].imshow(nue_event[0][0])\n",
        "\n",
        "nc_event = dataset.__getitem__(20000)\n",
        "print('True class:', nc_event[1])\n",
        "axes[2].imshow(nc_event[0][0])"
      ],
      "metadata": {
        "id": "5oKxh41q0Ikq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have loaded our dataset and are happy that it looks how we expect it to then we can move on and think about our networks.\n",
        "\n",
        "Let's start an MLP with three hidden layers with 256, 128 and 64 neurons, respectively. The final layer of the network has 3 neurons as those are the number of classes we are trying to identify in the dataset.In order to keep the number of parameters sensible, we will also downsample the image again by a factor of two.\n",
        "\n",
        "To fill in the blanks below, you'll need the following layers:\n",
        "* `torch.nn.MaxPool2d(kernel_size)` for the downsampling\n",
        "* `torch.nn.Flatten()` to go from 2D -> 1D\n",
        "* `torch.nn.Linear(in_features, out_features)` for the fully-connected layers\n",
        "* `torch.nn.ReLU()` for the activation function\n",
        "* `torch.nn.Dropout(p)` to apply fraction `p` dropout and prevent (hopefully) overtraining\n",
        "\n",
        "I've used the `torch.nn.Sequential` class here for convenience, but I'd typically define my model as a class that inherits from `torch.nn.Module`."
      ],
      "metadata": {
        "id": "z1abl8vivJyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start with an MLP\n",
        "mlp_model = torch.nn.Sequential(\n",
        "    # Downsample by a factor two\n",
        "    ...,\n",
        "    # Flatten ready for input to the Linear layer\n",
        "    ...,\n",
        "    # Linear layer with 256 outputs. Calculate the number of inputs!\n",
        "    ...,\n",
        "    # Apply the ReLU activation function\n",
        "    ...,\n",
        "    # Apply 50% dropout\n",
        "    ...,\n",
        "    # Linear layer with 256 inputs and 128 outputs\n",
        "    ...,\n",
        "    # Apply the ReLU activation function\n",
        "    ...,\n",
        "    # Another 50% dropout\n",
        "    ...,\n",
        "    # Linear layer with 128 inputs and 64 outputs\n",
        "    ...,\n",
        "    # Apply the ReLU activation function\n",
        "    ...,\n",
        "    # Another 50% dropout\n",
        "    ...,\n",
        "    # Output Linear layer with 64 inputs and n_classes outputs\n",
        "    ...\n",
        "    # We would expect to have a final torch.nn.Softmax activation layer here but\n",
        "    # for whatever reason there is an implicit one in the PyTorch implementation\n",
        "    # of the categorical cross-entropy loss function.\n",
        ")\n",
        "\n",
        "# Check that the model looks how we expect\n",
        "print(mlp_model)\n",
        "\n",
        "# A little bit of code to calculate the number of trainable parameters\n",
        "n_params = sum(p.numel() for p in mlp_model.parameters() if p.requires_grad)\n",
        "print('Number of trainable parameters =', n_params)"
      ],
      "metadata": {
        "id": "b42qOJca6hF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to define the loss function and optimiser. We need to use categorical cross-entropy loss and we'll choose the Adam optimiser.\n",
        "* `torch.nn.CrossEntropyLoss()`\n",
        "* `torch.nn.AdamW(params, lr)` where params are the model weights and lr is the learning rate"
      ],
      "metadata": {
        "id": "g90jJxDJ6RYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the learning rate to 1e-3\n",
        "learning_rate = 0.001\n",
        "# Use categorical cross-entropy loss\n",
        "mlp_loss_fn = ...\n",
        "# Use the AdamW optimiser and note that in the previous block of code we saw\n",
        "# how to access the model parameters\n",
        "mlp_optimiser = ..."
      ],
      "metadata": {
        "id": "0n8b1YnW8aEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block of code allows us to use a GPU if we have one available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using device', device)\n",
        "mlp_model.to(device)"
      ],
      "metadata": {
        "id": "CklD25ZVIU10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to train our network. The block of code is a bit lengthy, so please refer to the comments in the code below about what we need to do. That said, here are a couple of important things:\n",
        "* We run the model just by calling it like a function: `outputs = some_model(inputs)`\n",
        "* Similarly to calculate the loss: `loss = some_loss_function(outputs, labels)`"
      ],
      "metadata": {
        "id": "LG-vh75j9dIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs that we want to train for\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(0, n_epochs):\n",
        "  # We put the model into training mode (allows weights to be learned)\n",
        "  mlp_model.train()\n",
        "  running_loss = 0.0\n",
        "  # We iterate over each batch in the data loader\n",
        "  for (images, labels) in train_loader:\n",
        "    # Send the tensors to the correct device (CPU or GPU)\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass - we get a prediction and calculate the loss. Much easier\n",
        "    # than when we calculated it by hand in the lecture!\n",
        "    outputs = ...\n",
        "    loss = ...\n",
        "\n",
        "    # Backward pass - we make sure the gradients have been cleared and then\n",
        "    # do the back propagation\n",
        "    mlp_optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    mlp_optimiser.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  # Now we put the network in evaluation mode for the validation sample\n",
        "  running_val_loss = 0.0\n",
        "  mlp_model.eval()\n",
        "  # Disable gradients so that we don't waste time calculating them when not\n",
        "  # in training mode\n",
        "  with torch.no_grad():\n",
        "    # Iterate over the batches as before\n",
        "    for (images, labels) in val_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # All we need to do now is make the prediction and calculate the loss\n",
        "      outputs = ...\n",
        "      loss = ...\n",
        "      running_val_loss += loss.item()\n",
        "\n",
        "  print(\"Epoch\", epoch, \"training loss:\", running_loss/len(train_loader),\n",
        "        \"validation loss:\", running_val_loss/len(val_loader))"
      ],
      "metadata": {
        "id": "KvBpss3L9E92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next block just contains a couple of functions to allow us to find out which events we incorrectly classified and then have a look at them."
      ],
      "metadata": {
        "id": "y3iXaeEmrclB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a list of incorrect classifications\n",
        "def get_incorrect_classifications(model, dataloader):\n",
        "  incorrect_indices = []\n",
        "  with torch.no_grad():\n",
        "    for (images, labels) in dataloader:\n",
        "      images = images.to(device)\n",
        "      predictions = model(images).cpu().numpy()\n",
        "\n",
        "      for i in range(len(labels)):\n",
        "        prediction = np.argmax(predictions[i])\n",
        "        truth = labels[i].numpy()\n",
        "        if prediction != truth:\n",
        "          image = images[i].cpu().numpy()\n",
        "          image = image.transpose([1,2,0])\n",
        "          incorrect_indices.append([image, prediction, truth])\n",
        "\n",
        "  print('Accuracy =',1 - len(incorrect_indices)/len(val_idx))\n",
        "  return incorrect_indices\n",
        "\n",
        "def draw_event(incorrect_indices, index):\n",
        "  image_to_plot = incorrect_indices[index][0]\n",
        "  image_to_plot = np.clip(image_to_plot, 0.0, 1.0)\n",
        "  fig, ax = plt.subplots(1, 1)\n",
        "  print('Incorrect classification for image',index,\n",
        "        ': predicted =',incorrect_indices[index][1],\n",
        "        'with true =',incorrect_indices[index][2])\n",
        "  ax.imshow(image_to_plot)"
      ],
      "metadata": {
        "id": "YCIkcpRn6f4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we get the indices of the incorrectly classifed images"
      ],
      "metadata": {
        "id": "dCl4YERgrp5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the failures\n",
        "incorrect_indices = get_incorrect_classifications(mlp_model, val_loader)"
      ],
      "metadata": {
        "id": "m4DTY8IlpGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally we can draw some examples - the value here goes from zero to `len(incorrect_indices)`"
      ],
      "metadata": {
        "id": "4PUX2ayGrqgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "draw_event(incorrect_indices, 3)"
      ],
      "metadata": {
        "id": "HEhwtOTppLPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right, now that we've played a little with the MLP, lets get on with building a CNN. Many of the layers below will look familiar, but the one new one is:\n",
        "* `torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)`\n",
        "There are some additional arguments, but these are the important ones for now\n",
        "\n",
        "I have chosen the values of the stride (s) and padding (s) such that the size of the image (m) is reduced by a factor of exactly two after the convultion (n). The full equation for calculating the dimensions after a convolutional layer are: $n = \\frac{m - k + 2p}{s} + 1$, remembering that we are doing integer division here."
      ],
      "metadata": {
        "id": "WmcTq9FXsT2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets define our CNN network using the torch.nn.Sequential class\n",
        "cnn_model = torch.nn.Sequential(\n",
        "    # The 1st convolution has 32 (5x5) filters with stride 2 and padding 2\n",
        "    ...,\n",
        "    # ReLU activation function\n",
        "    ...,\n",
        "    # The 2nd convolution has 32 (3x3) filters with stride 2 and padding 1\n",
        "    ...,\n",
        "    # ReLU activation function\n",
        "    ...,\n",
        "    # The 3rd convolution has 32 (3x3) filters with stride 2 and padding 1\n",
        "    ...,\n",
        "    # ReLU activation function\n",
        "    ...,\n",
        "    # We need to flatten our tensor before the final layers\n",
        "    ...,\n",
        "    # Apply some dropout with probability 0.5\n",
        "    ...,\n",
        "    # The final classification layer with 3 outputs. The number of inputs needs\n",
        "    # to be calculated... think how big the 112 x 112 images is after the\n",
        "    # strided convolutions, and remember that we had 32 filters\n",
        "    ...\n",
        "    # Again, the final softmax is implicit in the loss function\n",
        ")\n",
        "\n",
        "print(cnn_model)\n",
        "\n",
        "n_params = sum(p.numel() for p in cnn_model.parameters() if p.requires_grad)\n",
        "print('Number of trainable parameters =', n_params)"
      ],
      "metadata": {
        "id": "0W-yxIN0XqvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the loss function and optimiser as we did before"
      ],
      "metadata": {
        "id": "FXPqtOEUysNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare for training\n",
        "learning_rate = 0.001\n",
        "cnn_loss_fn = torch.nn.CrossEntropyLoss()\n",
        "cnn_optimiser = torch.optim.AdamW(cnn_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "tETQjHcdb0qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send the model to the GPU if necessary"
      ],
      "metadata": {
        "id": "Jisn8vUt1qoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.to(device)"
      ],
      "metadata": {
        "id": "BLk0CMzgqR5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block is the same as when we trained the MLP (you'd probably write a function that does this generically for a given model, loss and otpimiser to save repeating code if this wasn't a quick tutorial)."
      ],
      "metadata": {
        "id": "RI6FYERg11hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(0, n_epochs):\n",
        "  cnn_model.train()\n",
        "  running_loss = 0.0\n",
        "  for (images, labels) in train_loader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = cnn_model(images)\n",
        "    loss = cnn_loss_fn(outputs, labels)\n",
        "\n",
        "    # Backward pass and optimisation\n",
        "    cnn_optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    cnn_optimiser.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  # Validation\n",
        "  running_val_loss = 0.0\n",
        "  cnn_model.eval()\n",
        "  with torch.no_grad():\n",
        "    for (images, labels) in val_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Make the predictions\n",
        "      outputs = cnn_model(images)\n",
        "      loss = cnn_loss_fn(outputs, labels)\n",
        "      running_val_loss += loss.item()\n",
        "\n",
        "  print(\"Epoch\", epoch, \"training loss:\", running_loss/len(train_loader), \"validation loss:\", running_val_loss/len(val_loader))"
      ],
      "metadata": {
        "id": "jJadaUMXqdMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at what we got wrong"
      ],
      "metadata": {
        "id": "BrWF4vjH2Ljp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the failures\n",
        "incorrect_indices = get_incorrect_classifications(cnn_model, val_loader)"
      ],
      "metadata": {
        "id": "TtMfhwNyKIgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And feel free to have a look at the failures!"
      ],
      "metadata": {
        "id": "H5V5DKYE2PKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "draw_event(incorrect_indices, 0)"
      ],
      "metadata": {
        "id": "9pK-SLXH-d0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "35atv2SUX4Au"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
